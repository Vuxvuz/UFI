# File: new_process.py - C·∫≠p nh·∫≠t v√† l√†m s·∫°ch n·ªôi dung t·ª´ Harvard Nutrition
import json
import asyncio
import time
import re
import os
import unicodedata
import csv
from collections import Counter, defaultdict
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from datetime import datetime
from typing import Dict, List, Set, Tuple, Any, Optional

# C·∫•u h√¨nh files
OUTPUT_FOLDER = './final_output'
INPUT_FILE = f'{OUTPUT_FOLDER}/cleaned_output.json'
OUTPUT_FILE = f'{OUTPUT_FOLDER}/updated_content.json'
CHECKPOINT_FILE = f'{OUTPUT_FOLDER}/crawler_checkpoint.json'
REMOVED_SENTENCES_FILE = f'{OUTPUT_FOLDER}/removed_sentences.csv'

# C·∫•u h√¨nh crawl
MAX_WORKERS = 5
MAX_RETRIES = 3
RETRY_DELAY = 2
PAGE_LOAD_TIMEOUT = 30000
BATCH_SIZE = 20

# C·∫•u h√¨nh x·ª≠ l√Ω n·ªôi dung
MIN_SENTENCE_LENGTH = 20
MAX_REPETITION_COUNT = 3
HEADER_CHECK_SENTENCES = 5  # S·ªë c√¢u ƒë·∫ßu ti√™n ki·ªÉm tra header spam

# ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# Danh s√°ch l∆∞u c√°c c√¢u b·ªã lo·∫°i b·ªè v√† l√Ω do
removed_sentences = []

def log_message(message):
    """Ghi log ra console v·ªõi timestamp"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def clean_text_minimal(text):
    """L√†m s·∫°ch text c∆° b·∫£n"""
    if not text:
        return ""
    text = re.sub(r'[\t\n\r]+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def preserve_layout_text(text):
    """Gi·ªØ c·∫•u tr√∫c v√† l√†m s·∫°ch text"""
    if not text:
        return ""
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'[ \t]+', ' ', text)
    
    # Chu·∫©n h√≥a c√°c d√≤ng tr·ªëng
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

def split_into_sentences(text):
    """T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u"""
    if not text:
        return []
    
    # S·ª≠ d·ª•ng regex ƒë·ªÉ t√°ch c√¢u
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    return sentences

def clean_header_spam(sentences):
    """Lo·∫°i b·ªè n·ªôi dung header spam ·ªü ƒë·∫ßu vƒÉn b·∫£n"""
    if not sentences or len(sentences) < 2:
        return sentences, 0
    
    header_removed = 0
    header_check_range = min(HEADER_CHECK_SENTENCES, len(sentences))
    
    # Ki·ªÉm tra c√°c c√¢u ƒë·∫ßu ti√™n
    for i in range(header_check_range):
        if '>' in sentences[i]:
            # L∆∞u c√¢u b·ªã lo·∫°i b·ªè
            for j in range(i+1):
                removed_sentences.append({
                    'sentence': sentences[j],
                    'reason': 'Truncated all content before last >'
                })
            
            # Ch·ªâ gi·ªØ l·∫°i n·ªôi dung t·ª´ c√¢u hi·ªán t·∫°i tr·ªü ƒëi
            header_removed = i + 1
            return sentences[i+1:], header_removed
    
    return sentences, header_removed

def detect_repeated_sentences(sentences):
    """Ph√°t hi·ªán v√† lo·∫°i b·ªè c√°c c√¢u l·∫∑p l·∫°i qu√° nhi·ªÅu l·∫ßn"""
    if not sentences:
        return sentences, 0
    
    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói c√¢u
    sentence_counts = Counter(sentences)
    
    # Danh s√°ch c√¢u sau khi l·ªçc
    filtered_sentences = []
    repetition_removed = 0
    
    # T·∫≠p h·ª£p c√°c c√¢u ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    processed_sentences = set()
    
    for sentence in sentences:
        # B·ªè qua c√¢u qu√° ng·∫Øn
        if len(sentence) < MIN_SENTENCE_LENGTH:
            continue
            
        # N·∫øu c√¢u xu·∫•t hi·ªán qu√° nhi·ªÅu l·∫ßn, ch·ªâ gi·ªØ l·∫°i m·ªôt s·ªë l∆∞·ª£ng nh·∫•t ƒë·ªãnh
        if sentence_counts[sentence] > MAX_REPETITION_COUNT:
            if sentence not in processed_sentences:
                # Th√™m v√†o c√¢u b·ªã lo·∫°i b·ªè v·ªõi l√Ω do
                removed_sentences.append({
                    'sentence': sentence,
                    'reason': 'Repeated >3 times'
                })
                
                # Th√™m v√†o t·∫≠p h·ª£p c√¢u ƒë√£ x·ª≠ l√Ω
                processed_sentences.add(sentence)
                
                # Ch·ªâ gi·ªØ l·∫°i MAX_REPETITION_COUNT c√¢u
                for _ in range(MAX_REPETITION_COUNT):
                    filtered_sentences.append(sentence)
                
                # ƒê·∫øm s·ªë c√¢u b·ªã lo·∫°i
                repetition_removed += sentence_counts[sentence] - MAX_REPETITION_COUNT
        else:
            filtered_sentences.append(sentence)
    
    return filtered_sentences, repetition_removed

def convert_to_markdown():
    """Tr·∫£ v·ªÅ m√£ JavaScript ƒë·ªÉ chuy·ªÉn ƒë·ªïi HTML sang Markdown"""
    js_convert_to_markdown = """
    (() => {
        function htmlToMarkdown(html) {
            let markdown = html;
            
            // Remove script and style tags
            markdown = markdown.replace(/<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, '');
            markdown = markdown.replace(/<style\b[^<]*(?:(?!<\/style>)<[^<]*)*<\/style>/gi, '');
            
            // Headings
            markdown = markdown.replace(/<h1[^>]*>(.*?)<\/h1>/gi, '# $1\\n\\n');
            markdown = markdown.replace(/<h2[^>]*>(.*?)<\/h2>/gi, '## $1\\n\\n');
            markdown = markdown.replace(/<h3[^>]*>(.*?)<\/h3>/gi, '### $1\\n\\n');
            markdown = markdown.replace(/<h4[^>]*>(.*?)<\/h4>/gi, '#### $1\\n\\n');
            markdown = markdown.replace(/<h5[^>]*>(.*?)<\/h5>/gi, '##### $1\\n\\n');
            markdown = markdown.replace(/<h6[^>]*>(.*?)<\/h6>/gi, '###### $1\\n\\n');
            
            // Paragraph
            markdown = markdown.replace(/<p[^>]*>(.*?)<\/p>/gi, '$1\\n\\n');
            
            // Bold and Italic
            markdown = markdown.replace(/<strong[^>]*>(.*?)<\/strong>/gi, '**$1**');
            markdown = markdown.replace(/<b[^>]*>(.*?)<\/b>/gi, '**$1**');
            markdown = markdown.replace(/<em[^>]*>(.*?)<\/em>/gi, '*$1*');
            markdown = markdown.replace(/<i[^>]*>(.*?)<\/i>/gi, '*$1*');
            
            // Links
            markdown = markdown.replace(/<a[^>]*href="(.*?)"[^>]*>(.*?)<\/a>/gi, '[$2]($1)');
            
            // Images
            markdown = markdown.replace(/<img[^>]*src="(.*?)"[^>]*alt="(.*?)"[^>]*>/gi, '![$2]($1)');
            markdown = markdown.replace(/<img[^>]*alt="(.*?)"[^>]*src="(.*?)"[^>]*>/gi, '![$1]($2)');
            markdown = markdown.replace(/<img[^>]*src="(.*?)"[^>]*>/gi, '![]($1)');
            
            // Lists
            markdown = markdown.replace(/<ul[^>]*>([\s\S]*?)<\/ul>/gi, function(match, content) {
                let listItems = '';
                content.replace(/<li[^>]*>([\s\S]*?)<\/li>/gi, function(match, item) {
                    listItems += '- ' + item.trim() + '\\n';
                    return '';
                });
                return listItems + '\\n';
            });
            
            markdown = markdown.replace(/<ol[^>]*>([\s\S]*?)<\/ol>/gi, function(match, content) {
                let listItems = '';
                let index = 1;
                content.replace(/<li[^>]*>([\s\S]*?)<\/li>/gi, function(match, item) {
                    listItems += index + '. ' + item.trim() + '\\n';
                    index++;
                    return '';
                });
                return listItems + '\\n';
            });
            
            // Blockquotes
            markdown = markdown.replace(/<blockquote[^>]*>([\s\S]*?)<\/blockquote>/gi, '> $1\\n\\n');
            
            // Code blocks
            markdown = markdown.replace(/<pre[^>]*><code[^>]*>([\s\S]*?)<\/code><\/pre>/gi, '```\\n$1\\n```\\n\\n');
            
            // Inline code
            markdown = markdown.replace(/<code[^>]*>(.*?)<\/code>/gi, '`$1`');
            
            // Horizontal rule
            markdown = markdown.replace(/<hr[^>]*>/gi, '---\\n\\n');
            
            // Tables
            markdown = markdown.replace(/<table[^>]*>([\s\S]*?)<\/table>/gi, function(match, tableContent) {
                let mdTable = '';
                
                // Process header row
                mdTable += tableContent.replace(/<thead[^>]*>([\s\S]*?)<\/thead>/gi, function(match, headContent) {
                    let header = '';
                    header += headContent.replace(/<tr[^>]*>([\s\S]*?)<\/tr>/gi, function(match, rowContent) {
                        let headerRow = '';
                        let separatorRow = '';
                        
                        rowContent.replace(/<th[^>]*>(.*?)<\/th>/gi, function(match, cellContent) {
                            headerRow += '| ' + cellContent.trim() + ' ';
                            separatorRow += '| --- ';
                            return '';
                        });
                        
                        headerRow += '|\\n';
                        separatorRow += '|\\n';
                        
                        return headerRow + separatorRow;
                    });
                    
                    return header;
                });
                
                // Process table body
                mdTable += tableContent.replace(/<tbody[^>]*>([\s\S]*?)<\/tbody>/gi, function(match, bodyContent) {
                    let bodyRows = '';
                    bodyRows += bodyContent.replace(/<tr[^>]*>(.*?)<\/tr>/gis, function(match, rowContent) {
                        let row = '';
                        row += rowContent.replace(/<td[^>]*>(.*?)<\/td>/gi, '$1 | ').trim();
                        row = '| ' + row + '\\n';
                        return row;
                    });
                    return bodyRows;
                });
                
                return mdTable + '\\n';
            });
            
            // Clean up HTML entities
            markdown = markdown.replace(/&nbsp;/g, ' ');
            markdown = markdown.replace(/&lt;/g, '<');
            markdown = markdown.replace(/&gt;/g, '>');
            markdown = markdown.replace(/&amp;/g, '&');
            markdown = markdown.replace(/&quot;/g, '"');
            
            // Clean up extra whitespace and newlines
            markdown = markdown.replace(/\\n\\s*\\n\\s*\\n/g, '\\n\\n');
            
            return markdown;
        }
        
        // Get the main content
        let content = document.body.innerHTML;
        
        // Try to find the main content container
        const possibleContainers = [
            document.querySelector('article'),
            document.querySelector('main'),
            document.querySelector('.content'),
            document.querySelector('.post-content'),
            document.querySelector('.entry-content'),
            document.querySelector('#content')
        ];
        
        for (const container of possibleContainers) {
            if (container) {
                content = container.innerHTML;
                break;
            }
        }
        
        return htmlToMarkdown(content);
    })();
    """
    
    return js_convert_to_markdown

async def extract_content(page):
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ trang web v√† chuy·ªÉn ƒë·ªïi sang Markdown"""
    try:
        # Chuy·ªÉn ƒë·ªïi HTML sang Markdown
        markdown_content = await page.evaluate(convert_to_markdown())
        
        # L√†m s·∫°ch n·ªôi dung Markdown
        cleaned_content = preserve_layout_text(markdown_content)
        
        # Chia th√†nh c√°c c√¢u
        sentences = split_into_sentences(cleaned_content)
        
        # Lo·∫°i b·ªè header spam
        cleaned_sentences, header_removed = clean_header_spam(sentences)
        
        # Lo·∫°i b·ªè c√¢u l·∫∑p l·∫°i
        cleaned_sentences, repetition_removed = detect_repeated_sentences(cleaned_sentences)
        
        # Gh√©p l·∫°i c√°c c√¢u th√†nh n·ªôi dung ho√†n ch·ªânh
        final_content = ' '.join(cleaned_sentences)
        
        return {
            'content': final_content,
            'removed_count': header_removed + repetition_removed
        }
        
    except Exception as e:
        log_message(f"‚ö†Ô∏è L·ªói khi tr√≠ch xu·∫•t n·ªôi dung: {str(e)}")
        return {
            'content': '',
            'removed_count': 0
        }

async def process_article(context, entry, semaphore):
    """X·ª≠ l√Ω m·ªôt b√†i vi·∫øt v√† tr√≠ch xu·∫•t n·ªôi dung"""
    url = entry.get('url', '')
    title = entry.get('title', '')
    category = entry.get('category', '')
    
    # T·∫°o b·∫£n sao c·ªßa entry ƒë·ªÉ c·∫≠p nh·∫≠t
    updated_entry = entry.copy()
    
    if not url:
        log_message(f"‚ö†Ô∏è B·ªè qua m·ª•c kh√¥ng c√≥ URL: {title}")
        return updated_entry
    
    # S·ª≠ d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi
    async with semaphore:
        # Ki·ªÉm tra URL h·ª£p l·ªá
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            log_message(f"‚ö†Ô∏è URL kh√¥ng h·ª£p l·ªá: {url}")
            return updated_entry
        
        retries = 0
        while retries < MAX_RETRIES:
            try:
                log_message(f"üîç ƒêang x·ª≠ l√Ω: {title} ({url})")
                
                # T·∫°o page m·ªõi
                page = await context.new_page()
                
                try:
                    # Thi·∫øt l·∫≠p timeout
                    page.set_default_timeout(PAGE_LOAD_TIMEOUT)
                    
                    # Truy c·∫≠p URL
                    await page.goto(url, wait_until='domcontentloaded')
                    
                    # ƒê·ª£i trang t·∫£i xong
                    await page.wait_for_load_state('networkidle')
                    
                    # Tr√≠ch xu·∫•t n·ªôi dung
                    result = await extract_content(page)
                    content = result['content']
                    removed_count = result['removed_count']
                    
                    if content:
                        log_message(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t n·ªôi dung ({len(content)} k√Ω t·ª±, lo·∫°i b·ªè {removed_count} c√¢u): {title}")
                        updated_entry['content'] = content
                        updated_entry['last_updated'] = datetime.now().isoformat()
                        updated_entry['removed_sentences_count'] = removed_count
                    else:
                        log_message(f"‚ö†Ô∏è Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung: {title}")
                    
                    # ƒê√≥ng page
                    await page.close()
                    
                    # Tr·∫£ v·ªÅ k·∫øt qu·∫£
                    return updated_entry
                    
                except Exception as e:
                    # ƒê√≥ng page n·∫øu c√≥ l·ªói
                    await page.close()
                    raise e
                
            except Exception as e:
                retries += 1
                log_message(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω {url} (l·∫ßn {retries}/{MAX_RETRIES}): {str(e)}")
                
                if retries < MAX_RETRIES:
                    log_message(f"üîÑ Th·ª≠ l·∫°i sau {RETRY_DELAY} gi√¢y...")
                    await asyncio.sleep(RETRY_DELAY)
                else:
                    log_message(f"‚ùå ƒê√£ v∆∞·ª£t qu√° s·ªë l·∫ßn th·ª≠ l·∫°i cho {url}")
                    return updated_entry

async def main():
    """H√†m ch√≠nh th·ª±c hi·ªán qu√° tr√¨nh c·∫≠p nh·∫≠t n·ªôi dung"""
    log_message("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh c·∫≠p nh·∫≠t n·ªôi dung...")
    start_time = time.time()
    
    try:
        # ƒê·ªçc d·ªØ li·ªáu ƒë·∫ßu v√†o
        log_message(f"üìñ ƒê·ªçc d·ªØ li·ªáu t·ª´ {INPUT_FILE}...")
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        log_message(f"üìä ƒê√£ ƒë·ªçc {len(data)} b·∫£n ghi.")
        
        # Kh·ªüi t·∫°o playwright
        log_message("üîß Kh·ªüi t·∫°o tr√¨nh duy·ªát...")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            )
            
            # Chia th√†nh c√°c batch ƒë·ªÉ x·ª≠ l√Ω
            batches = [data[i:i+BATCH_SIZE] for i in range(0, len(data), BATCH_SIZE)]
            total_batches = len(batches)
            
            # Thi·∫øt l·∫≠p semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi
            semaphore = asyncio.Semaphore(MAX_WORKERS)
            
            updated_data = []
            
            for batch_idx, batch in enumerate(batches):
                log_message(f"üîÑ ƒêang x·ª≠ l√Ω batch {batch_idx+1}/{total_batches}...")
                
                # X·ª≠ l√Ω song song c√°c URL trong batch
                tasks = [process_article(context, entry, semaphore) for entry in batch]
                batch_results = await asyncio.gather(*tasks)
                
                # Th√™m v√†o k·∫øt qu·∫£
                updated_data.extend(batch_results)
                
                # L∆∞u checkpoint
                with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
                    checkpoint_data = {
                        'processed_count': len(updated_data),
                        'total_count': len(data),
                        'last_batch': batch_idx,
                        'timestamp': datetime.now().isoformat()
                    }
                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
                # Delay gi·ªØa c√°c batch ƒë·ªÉ gi·∫£m t·∫£i cho server
                if batch_idx < total_batches - 1:
                    await asyncio.sleep(1)
            
            await browser.close()
        
        # L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng
        log_message("üíæ L∆∞u k·∫øt qu·∫£ c·∫≠p nh·∫≠t...")
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(updated_data, f, indent=2, ensure_ascii=False)
        
        # L∆∞u log c√°c c√¢u b·ªã lo·∫°i b·ªè
        log_message(f"üìù L∆∞u log c√°c c√¢u b·ªã lo·∫°i b·ªè v√†o {REMOVED_SENTENCES_FILE}...")
        with open(REMOVED_SENTENCES_FILE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['sentence', 'reason'])
            for item in removed_sentences:
                writer.writerow([item['sentence'], item['reason']])
        
        # T·ªïng k·∫øt
        total_removed = sum(entry.get('removed_sentences_count', 0) for entry in updated_data)
        elapsed_time = time.time() - start_time
        log_message(f"‚úÖ Qu√° tr√¨nh c·∫≠p nh·∫≠t ho√†n t·∫•t trong {elapsed_time:.2f} gi√¢y ({elapsed_time/60:.2f} ph√∫t)")
        log_message(f"üìä T·ªïng k·∫øt:")
        log_message(f"   - T·ªïng s·ªë b·∫£n ghi ƒë√£ x·ª≠ l√Ω: {len(updated_data)}")
        log_message(f"   - T·ªïng s·ªë c√¢u b·ªã lo·∫°i b·ªè: {total_removed}")
        log_message(f"   - K·∫øt qu·∫£ ƒë√£ l∆∞u v√†o: {OUTPUT_FILE}")
        log_message(f"   - Log c√¢u b·ªã lo·∫°i b·ªè: {REMOVED_SENTENCES_FILE}")
        
    except Exception as e:
        log_message(f"‚ùå L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())