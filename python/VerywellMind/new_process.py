#!/usr/bin/env python3
"""
Verywell Mind Crawler - Phase 2: Thu th·∫≠p n·ªôi dung chi ti·∫øt t·ª´ c√°c b√†i vi·∫øt
"""

import json
import logging
import asyncio
import aiofiles
import re
import os
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright
import time
from pathlib import Path

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl_phase2.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContentCrawler:
    def __init__(self):
        self.input_file = './verywellmind_data.json'
        self.output_json = './verywellmind_full_content.json'
        self.output_text_dir = './verywellmind_texts'
        self.concurrency = 5
        self.results = []
        self.stats = {
            'total_main_pages': 0,
            'total_sub_articles': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'start_time': None,
            'end_time': None,
            'category_stats': {}
        }

    async def crawl_content(self):
        """Thu th·∫≠p n·ªôi dung chi ti·∫øt t·ª´ t·∫•t c·∫£ c√°c trang"""
        logger.info("üöÄ B·∫Øt ƒë·∫ßu Phase 2: Thu th·∫≠p n·ªôi dung chi ti·∫øt")
        self.stats['start_time'] = time.time()
        
        # T·∫°o th∆∞ m·ª•c output
        Path(self.output_text_dir).mkdir(exist_ok=True)
        
        # ƒê·ªçc d·ªØ li·ªáu input
        input_data = await self._load_input_data()
        if not input_data:
            return
        
        # Crawl v·ªõi concurrency control
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            # T·∫°o semaphore ƒë·ªÉ control concurrency
            semaphore = asyncio.Semaphore(self.concurrency)
            
            # T·∫°o tasks cho t·∫•t c·∫£ main pages
            tasks = [
                self._process_main_page(context, item, semaphore) 
                for item in input_data
            ]
            
            # Ch·∫°y t·∫•t c·∫£ tasks
            await asyncio.gather(*tasks, return_exceptions=True)
            
            await browser.close()
        
        self.stats['end_time'] = time.time()
        await self._save_results()
        self._print_summary()

    async def _load_input_data(self):
        """ƒê·ªçc d·ªØ li·ªáu t·ª´ file input"""
        try:
            async with aiofiles.open(self.input_file, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                
                # X·ª≠ l√Ω c·∫£ format c≈© v√† m·ªõi
                if isinstance(data, dict) and 'data' in data:
                    input_data = data['data']
                else:
                    input_data = data
                
                self.stats['total_main_pages'] = len(input_data)
                logger.info(f"üìñ ƒê√£ t·∫£i {len(input_data)} main pages t·ª´ {self.input_file}")
                return input_data
                
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file input {self.input_file}: {str(e)}")
            return None

    async def _process_main_page(self, context, item, semaphore):
        """X·ª≠ l√Ω m·ªôt main page v√† t·∫•t c·∫£ sub-articles c·ªßa n√≥"""
        async with semaphore:
            page_title = item.get('title', 'Unknown')
            page_url = item.get('url', '')
            page_category = item.get('category', 'general')
            
            logger.info(f"üîç ƒêang x·ª≠ l√Ω: {page_title} ({page_category})")
            
            page = await context.new_page()
            try:
                await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                
                # T√¨m t·∫•t c·∫£ c√°c article links trong trang
                sub_links = await self._extract_article_links(page, page_url)
                logger.info(f"   üìÑ T√¨m th·∫•y {len(sub_links)} sub-articles")
                
                sub_articles = []
                for link in sub_links:
                    sub_content = await self._crawl_article_content(context, link, page_category)
                    if sub_content:
                        sub_articles.append(sub_content)
                
                # L∆∞u k·∫øt qu·∫£ cho main page n√†y
                result = {
                    'title': page_title,
                    'url': page_url,
                    'category': page_category,
                    'sub_articles': sub_articles,
                    'total_sub_articles': len(sub_articles)
                }
                
                self.results.append(result)
                self.stats['successful_pages'] += 1
                self.stats['total_sub_articles'] += len(sub_articles)
                
                # C·∫≠p nh·∫≠t category stats
                if page_category not in self.stats['category_stats']:
                    self.stats['category_stats'][page_category] = {'pages': 0, 'articles': 0}
                self.stats['category_stats'][page_category]['pages'] += 1
                self.stats['category_stats'][page_category]['articles'] += len(sub_articles)
                
                logger.info(f"‚úÖ Ho√†n th√†nh {page_title}: {len(sub_articles)} articles")
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {page_url}: {str(e)}")
                self.stats['failed_pages'] += 1
            finally:
                await page.close()

    async def _extract_article_links(self, page, base_url):
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ article links t·ª´ m·ªôt trang"""
        try:
            # T√¨m t·∫•t c·∫£ links trong trang
            all_links = await page.evaluate("""
                () => {
                    const links = Array.from(document.querySelectorAll('a[href]'));
                    return links.map(a => a.href).filter(href => 
                        href && 
                        href.startsWith('https://www.verywellmind.com') && 
                        !href.includes('#') &&
                        !href.includes('?') &&
                        href !== window.location.href
                    );
                }
            """)
            
            # Lo·∫°i b·ªè duplicates v√† filter
            unique_links = list(set(all_links))
            
            # Filter c√°c link kh√¥ng ph·∫£i article
            filtered_links = []
            for link in unique_links:
                if self._is_article_link(link):
                    filtered_links.append(link)
            
            return filtered_links[:50]  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh qu√° t·∫£i
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi extract links: {str(e)}")
            return []

    def _is_article_link(self, url):
        """Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† article link kh√¥ng"""
        # Lo·∫°i b·ªè c√°c URL kh√¥ng ph·∫£i article
        exclude_patterns = [
            '/about-us/', '/privacy-policy/', '/terms-of-service/',
            '/newsletter/', '/contact/', '/careers/', '/advertise/',
            '/crisis-support/', '/review-board/', '/editorial-process/',
            '/authors/', '/fact-checking/', '/medical-review-board/'
        ]
        
        url_lower = url.lower()
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return False
        
        # Ch·ªâ l·∫•y c√°c URL c√≥ pattern nh∆∞ article
        article_patterns = [
            r'/[a-zA-Z0-9-]+\d+$',  # URL k·∫øt th√∫c b·∫±ng s·ªë (th∆∞·ªùng l√† article ID)
            r'/[a-zA-Z0-9-]+-\d+$',  # URL c√≥ pattern name-id
        ]
        
        for pattern in article_patterns:
            if re.search(pattern, url):
                return True
        
        # N·∫øu URL c√≥ ƒë·ªô d√†i h·ª£p l√Ω v√† ch·ª©a t·ª´ kh√≥a article
        if len(url.split('/')) >= 4 and any(keyword in url_lower for keyword in [
            'anxiety', 'depression', 'stress', 'therapy', 'mental', 'health',
            'psychology', 'wellness', 'condition', 'symptom', 'treatment'
        ]):
            return True
        
        return False

    async def _crawl_article_content(self, context, url, category):
        """Thu th·∫≠p n·ªôi dung t·ª´ m·ªôt article"""
        page = await context.new_page()
        try:
            logger.info(f"   ‚Ü™ ƒêang crawl: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # Tr√≠ch xu·∫•t n·ªôi dung article
            content_data = await page.evaluate("""
                () => {
                    // T√¨m element ch·ª©a n·ªôi dung ch√≠nh
                    const selectors = [
                        'article',
                        '.article-content',
                        '.content',
                        '[data-testid="article-content"]',
                        '.post-content',
                        'main'
                    ];
                    
                    let content = '';
                    let title = '';
                    
                    // L·∫•y title
                    const titleSelectors = ['h1', '.article-title', '.post-title', 'title'];
                    for (const sel of titleSelectors) {
                        const elem = document.querySelector(sel);
                        if (elem && elem.textContent.trim()) {
                            title = elem.textContent.trim();
                            break;
                        }
                    }
                    
                    // L·∫•y content
                    for (const sel of selectors) {
                        const elem = document.querySelector(sel);
                        if (elem) {
                            content = elem.innerText || elem.textContent || '';
                            if (content.length > 200) {  // Ch·ªâ l·∫•y n·∫øu c√≥ ƒë·ªß content
                                break;
                            }
                        }
                    }
                    
                    // Fallback: l·∫•y t·ª´ body
                    if (!content || content.length < 200) {
                        content = document.body.innerText || document.body.textContent || '';
                    }
                    
                    return {
                        title: title,
                        content: content.trim()
                    };
                }
            """)
            
            if content_data['content'] and len(content_data['content']) > 100:
                # L√†m s·∫°ch content
                cleaned_content = self._clean_content(content_data['content'])
                
                # T·∫°o t√™n file an to√†n
                safe_filename = self._create_safe_filename(url)
                text_file_path = os.path.join(self.output_text_dir, f"{safe_filename}.txt")
                
                # L∆∞u file text
                async with aiofiles.open(text_file_path, 'w', encoding='utf-8') as f:
                    await f.write(cleaned_content)
                
                return {
                    'title': content_data['title'] or 'Untitled',
                    'url': url,
                    'content': cleaned_content,
                    'category': category,
                    'word_count': len(cleaned_content.split()),
                    'text_file': text_file_path
                }
            else:
                logger.warning(f"‚ö†Ô∏è N·ªôi dung qu√° ng·∫Øn ho·∫∑c tr·ªëng: {url}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi crawl article {url}: {str(e)}")
            return None
        finally:
            await page.close()

    def _clean_content(self, content):
        """L√†m s·∫°ch n·ªôi dung, x·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát"""
        if not content:
            return ""
        
        # X·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
        content = content.replace('\t', ' ')  # Tab -> space
        content = content.replace('\r\n', '\n')  # Windows line endings
        content = content.replace('\r', '\n')  # Mac line endings
        content = content.replace('\u00a0', ' ')  # Non-breaking space
        content = content.replace('\u2019', "'")  # Smart apostrophe
        content = content.replace('\u2018', "'")  # Smart apostrophe
        content = content.replace('\u201c', '"')  # Smart quote
        content = content.replace('\u201d', '"')  # Smart quote
        content = content.replace('\u2013', '-')  # En dash
        content = content.replace('\u2014', '-')  # Em dash
        
        # Lo·∫°i b·ªè multiple spaces v√† newlines
        content = re.sub(r' +', ' ', content)  # Multiple spaces -> single space
        content = re.sub(r'\n{3,}', '\n\n', content)  # Multiple newlines -> double newline
        
        return content.strip()

    def _create_safe_filename(self, url):
        """T·∫°o t√™n file an to√†n t·ª´ URL"""
        # L·∫•y ph·∫ßn cu·ªëi c·ªßa URL
        filename = url.split('/')[-1]
        if not filename:
            filename = url.split('/')[-2]
        
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá
        filename = re.sub(r'[^\w\-_.]', '_', filename)
        filename = re.sub(r'_+', '_', filename)  # Multiple underscores -> single
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename.lower()

    async def _save_results(self):
        """L∆∞u k·∫øt qu·∫£ v√†o file JSON"""
        try:
            output_data = {
                'metadata': {
                    'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_main_pages': len(self.results),
                    'total_sub_articles': self.stats['total_sub_articles'],
                    'stats': self.stats
                },
                'data': self.results
            }
            
            async with aiofiles.open(self.output_json, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(output_data, indent=2, ensure_ascii=False))
            
            logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {self.output_json}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£: {str(e)}")

    def _print_summary(self):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("\n" + "="*60)
        logger.info("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ CRAWL PHASE 2")
        logger.info("="*60)
        logger.info(f"‚è±Ô∏è  Th·ªùi gian th·ª±c hi·ªán: {duration:.2f} gi√¢y")
        logger.info(f"üìÑ T·ªïng main pages: {self.stats['total_main_pages']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {self.stats['successful_pages']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {self.stats['failed_pages']}")
        logger.info(f"üì∞ T·ªïng sub-articles: {self.stats['total_sub_articles']}")
        
        logger.info("\nüìä TH·ªêNG K√ä THEO CATEGORY:")
        for category, stats in self.stats['category_stats'].items():
            logger.info(f"   {category}: {stats['pages']} pages, {stats['articles']} articles")
        
        logger.info(f"\nüìÅ File texts ƒë√£ l∆∞u trong: {self.output_text_dir}")
        logger.info("="*60)

async def main():
    crawler = ContentCrawler()
    await crawler.crawl_content()

if __name__ == "__main__":
    asyncio.run(main())