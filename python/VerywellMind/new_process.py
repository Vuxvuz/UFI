#!/usr/bin/env python3
"""
Verywell Mind Crawler - Phase 2: Thu tháº­p ná»™i dung chi tiáº¿t tá»« cÃ¡c bÃ i viáº¿t
"""

import json
import logging
import asyncio
import aiofiles
import re
import os
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright
import time
from pathlib import Path

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl_phase2.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContentCrawler:
    def __init__(self):
        self.input_file = './verywellmind_data.json'
        self.output_json = './verywellmind_full_content.json'
        self.output_text_dir = './verywellmind_texts'
        self.concurrency = 5
        self.results = []
        self.stats = {
            'total_main_pages': 0,
            'total_sub_articles': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'start_time': None,
            'end_time': None,
            'category_stats': {}
        }

    async def crawl_content(self):
        """Thu tháº­p ná»™i dung chi tiáº¿t tá»« táº¥t cáº£ cÃ¡c trang"""
        logger.info("ğŸš€ Báº¯t Ä‘áº§u Phase 2: Thu tháº­p ná»™i dung chi tiáº¿t")
        self.stats['start_time'] = time.time()
        
        # Táº¡o thÆ° má»¥c output
        Path(self.output_text_dir).mkdir(exist_ok=True)
        
        # Äá»c dá»¯ liá»‡u input
        input_data = await self._load_input_data()
        if not input_data:
            return
        
        # Crawl vá»›i concurrency control
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            # Táº¡o semaphore Ä‘á»ƒ control concurrency
            semaphore = asyncio.Semaphore(self.concurrency)
            
            # Táº¡o tasks cho táº¥t cáº£ main pages
            tasks = [
                self._process_main_page(context, item, semaphore) 
                for item in input_data
            ]
            
            # Cháº¡y táº¥t cáº£ tasks
            await asyncio.gather(*tasks, return_exceptions=True)
            
            await browser.close()
        
        self.stats['end_time'] = time.time()
        await self._save_results()
        self._print_summary()

    async def _load_input_data(self):
        """Äá»c dá»¯ liá»‡u tá»« file input"""
        try:
            async with aiofiles.open(self.input_file, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                
                # Xá»­ lÃ½ cáº£ format cÅ© vÃ  má»›i
                if isinstance(data, dict) and 'data' in data:
                    input_data = data['data']
                else:
                    input_data = data
                
                self.stats['total_main_pages'] = len(input_data)
                logger.info(f"ğŸ“– ÄÃ£ táº£i {len(input_data)} main pages tá»« {self.input_file}")
                return input_data
                
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c file input {self.input_file}: {str(e)}")
            return None

    async def _process_main_page(self, context, item, semaphore):
        """Xá»­ lÃ½ má»™t main page vÃ  táº¥t cáº£ sub-articles cá»§a nÃ³"""
        async with semaphore:
            page_title = item.get('title', 'Unknown')
            page_url = item.get('url', '')
            page_category = item.get('category', 'general')
            
            logger.info(f"ğŸ” Äang xá»­ lÃ½: {page_title} ({page_category})")
            
            page = await context.new_page()
            try:
                await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                
                # TÃ¬m táº¥t cáº£ cÃ¡c article links trong trang
                sub_links = await self._extract_article_links(page, page_url)
                logger.info(f"   ğŸ“„ TÃ¬m tháº¥y {len(sub_links)} sub-articles")
                
                sub_articles = []
                for link in sub_links:
                    sub_content = await self._crawl_article_content(context, link, page_category)
                    if sub_content:
                        sub_articles.append(sub_content)
                
                # LÆ°u káº¿t quáº£ cho main page nÃ y
                result = {
                    'title': page_title,
                    'url': page_url,
                    'category': page_category,
                    'sub_articles': sub_articles,
                    'total_sub_articles': len(sub_articles)
                }
                
                self.results.append(result)
                self.stats['successful_pages'] += 1
                self.stats['total_sub_articles'] += len(sub_articles)
                
                # Cáº­p nháº­t category stats
                if page_category not in self.stats['category_stats']:
                    self.stats['category_stats'][page_category] = {'pages': 0, 'articles': 0}
                self.stats['category_stats'][page_category]['pages'] += 1
                self.stats['category_stats'][page_category]['articles'] += len(sub_articles)
                
                logger.info(f"âœ… HoÃ n thÃ nh {page_title}: {len(sub_articles)} articles")
                
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi xá»­ lÃ½ {page_url}: {str(e)}")
                self.stats['failed_pages'] += 1
            finally:
                await page.close()

    async def _extract_article_links(self, page, base_url):
        """TrÃ­ch xuáº¥t táº¥t cáº£ article links tá»« má»™t trang"""
        try:
            # TÃ¬m táº¥t cáº£ links trong trang
            all_links = await page.evaluate("""
                () => {
                    const links = Array.from(document.querySelectorAll('a[href]'));
                    return links.map(a => a.href).filter(href => 
                        href && 
                        href.startsWith('https://www.verywellmind.com') && 
                        !href.includes('#') &&
                        !href.includes('?') &&
                        href !== window.location.href
                    );
                }
            """)
            
            # Loáº¡i bá» duplicates vÃ  filter
            unique_links = list(set(all_links))
            
            # Filter cÃ¡c link khÃ´ng pháº£i article
            filtered_links = []
            for link in unique_links:
                if self._is_article_link(link):
                    filtered_links.append(link)
            
            return filtered_links[:50]  # Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i
            
        except Exception as e:
            logger.warning(f"âš ï¸ Lá»—i khi extract links: {str(e)}")
            return []

    def _is_article_link(self, url):
        """Kiá»ƒm tra xem URL cÃ³ pháº£i lÃ  article link khÃ´ng"""
        # Loáº¡i bá» cÃ¡c URL khÃ´ng pháº£i article
        exclude_patterns = [
            '/about-us/', '/privacy-policy/', '/terms-of-service/',
            '/newsletter/', '/contact/', '/careers/', '/advertise/',
            '/crisis-support/', '/review-board/', '/editorial-process/',
            '/authors/', '/fact-checking/', '/medical-review-board/'
        ]
        
        url_lower = url.lower()
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return False
        
        # Chá»‰ láº¥y cÃ¡c URL cÃ³ pattern nhÆ° article
        article_patterns = [
            r'/[a-zA-Z0-9-]+\d+$',  # URL káº¿t thÃºc báº±ng sá»‘ (thÆ°á»ng lÃ  article ID)
            r'/[a-zA-Z0-9-]+-\d+$',  # URL cÃ³ pattern name-id
        ]
        
        for pattern in article_patterns:
            if re.search(pattern, url):
                return True
        
        # Náº¿u URL cÃ³ Ä‘á»™ dÃ i há»£p lÃ½ vÃ  chá»©a tá»« khÃ³a article
        if len(url.split('/')) >= 4 and any(keyword in url_lower for keyword in [
            'anxiety', 'depression', 'stress', 'therapy', 'mental', 'health',
            'psychology', 'wellness', 'condition', 'symptom', 'treatment'
        ]):
            return True
        
        return False

    async def _crawl_article_content(self, context, url, category):
        """Thu tháº­p ná»™i dung tá»« má»™t article"""
        page = await context.new_page()
        try:
            logger.info(f"   â†ª Äang crawl: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # TrÃ­ch xuáº¥t ná»™i dung article
            content_data = await page.evaluate("""
                () => {
                    // TÃ¬m element chá»©a ná»™i dung chÃ­nh
                    const selectors = [
                        'article',
                        '.article-content',
                        '.content',
                        '[data-testid="article-content"]',
                        '.post-content',
                        'main'
                    ];
                    
                    let content = '';
                    let title = '';
                    
                    // Láº¥y title
                    const titleSelectors = ['h1', '.article-title', '.post-title', 'title'];
                    for (const sel of titleSelectors) {
                        const elem = document.querySelector(sel);
                        if (elem && elem.textContent.trim()) {
                            title = elem.textContent.trim();
                            break;
                        }
                    }
                    
                    // Láº¥y content
                    for (const sel of selectors) {
                        const elem = document.querySelector(sel);
                        if (elem) {
                            content = elem.innerText || elem.textContent || '';
                            if (content.length > 200) {  // Chá»‰ láº¥y náº¿u cÃ³ Ä‘á»§ content
                                break;
                            }
                        }
                    }
                    
                    // Fallback: láº¥y tá»« body
                    if (!content || content.length < 200) {
                        content = document.body.innerText || document.body.textContent || '';
                    }
                    
                    return {
                        title: title,
                        content: content.trim()
                    };
                }
            """)
            
            if content_data['content'] and len(content_data['content']) > 100:
                # LÃ m sáº¡ch content
                cleaned_content = self._clean_content(content_data['content'])
                
                # Táº¡o tÃªn file an toÃ n
                safe_filename = self._create_safe_filename(url)
                text_file_path = os.path.join(self.output_text_dir, f"{safe_filename}.txt")
                
                # LÆ°u file text
                async with aiofiles.open(text_file_path, 'w', encoding='utf-8') as f:
                    await f.write(cleaned_content)
                
                return {
                    'title': content_data['title'] or 'Untitled',
                    'url': url,
                    'content': cleaned_content,
                    'category': category,
                    'word_count': len(cleaned_content.split()),
                    'text_file': text_file_path
                }
            else:
                logger.warning(f"âš ï¸ Ná»™i dung quÃ¡ ngáº¯n hoáº·c trá»‘ng: {url}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi crawl article {url}: {str(e)}")
            return None
        finally:
            await page.close()

    def _clean_content(self, content):
        """LÃ m sáº¡ch ná»™i dung, xá»­ lÃ½ kÃ½ tá»± Ä‘áº·c biá»‡t"""
        if not content:
            return ""
        
        # Xá»­ lÃ½ cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
        content = content.replace('\t', ' ')  # Tab -> space
        content = content.replace('\r\n', '\n')  # Windows line endings
        content = content.replace('\r', '\n')  # Mac line endings
        content = content.replace('\u00a0', ' ')  # Non-breaking space
        content = content.replace('\u2019', "'")  # Smart apostrophe
        content = content.replace('\u2018', "'")  # Smart apostrophe
        content = content.replace('\u201c', '"')  # Smart quote
        content = content.replace('\u201d', '"')  # Smart quote
        content = content.replace('\u2013', '-')  # En dash
        content = content.replace('\u2014', '-')  # Em dash
        
        # Loáº¡i bá» multiple spaces vÃ  newlines
        content = re.sub(r' +', ' ', content)  # Multiple spaces -> single space
        content = re.sub(r'\n{3,}', '\n\n', content)  # Multiple newlines -> double newline
        
        return content.strip()

    def _create_safe_filename(self, url):
        """Táº¡o tÃªn file an toÃ n tá»« URL"""
        # Láº¥y pháº§n cuá»‘i cá»§a URL
        filename = url.split('/')[-1]
        if not filename:
            filename = url.split('/')[-2]
        
        # Loáº¡i bá» kÃ½ tá»± khÃ´ng há»£p lá»‡
        filename = re.sub(r'[^\w\-_.]', '_', filename)
        filename = re.sub(r'_+', '_', filename)  # Multiple underscores -> single
        
        # Giá»›i háº¡n Ä‘á»™ dÃ i
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename.lower()

    async def _save_results(self):
        """LÆ°u káº¿t quáº£ vÃ o file JSON"""
        try:
            output_data = {
                'metadata': {
                    'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_main_pages': len(self.results),
                    'total_sub_articles': self.stats['total_sub_articles'],
                    'stats': self.stats
                },
                'data': self.results
            }
            
            async with aiofiles.open(self.output_json, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(output_data, indent=2, ensure_ascii=False))
            
            logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o {self.output_json}")
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi lÆ°u káº¿t quáº£: {str(e)}")

    def _print_summary(self):
        """In tÃ³m táº¯t káº¿t quáº£"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢ CRAWL PHASE 2")
        logger.info("="*60)
        logger.info(f"â±ï¸  Thá»i gian thá»±c hiá»‡n: {duration:.2f} giÃ¢y")
        logger.info(f"ğŸ“„ Tá»•ng main pages: {self.stats['total_main_pages']}")
        logger.info(f"âœ… ThÃ nh cÃ´ng: {self.stats['successful_pages']}")
        logger.info(f"âŒ Tháº¥t báº¡i: {self.stats['failed_pages']}")
        logger.info(f"ğŸ“° Tá»•ng sub-articles: {self.stats['total_sub_articles']}")
        
        logger.info("\nğŸ“Š THá»NG KÃŠ THEO CATEGORY:")
        for category, stats in self.stats['category_stats'].items():
            logger.info(f"   {category}: {stats['pages']} pages, {stats['articles']} articles")
        
        logger.info(f"\nğŸ“ File texts Ä‘Ã£ lÆ°u trong: {self.output_text_dir}")
        logger.info("="*60)

async def main():
    crawler = ContentCrawler()
    await crawler.crawl_content()

if __name__ == "__main__":
    asyncio.run(main())