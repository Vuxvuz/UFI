#!/usr/bin/env python3
"""
Verywell Mind Crawler - Phase 1: Thu th·∫≠p danh s√°ch c√°c ch·ªß ƒë·ªÅ
"""

import json
import logging
import asyncio
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright
import time

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl_phase1.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class VerywellMindCrawler:
    def __init__(self):
        self.base_url = 'https://www.verywellmind.com/'
        self.results = []
        self.stats = {
            'total_items': 0,
            'tag_items': 0,
            'alphabetical_items': 0,
            'errors': 0,
            'start_time': None,
            'end_time': None
        }

    async def crawl_categories(self):
        """Thu th·∫≠p t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ trang ch·ªß"""
        logger.info("üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p danh s√°ch ch·ªß ƒë·ªÅ t·ª´ Verywell Mind")
        self.stats['start_time'] = time.time()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                logger.info(f"üìñ Truy c·∫≠p trang ch·ªß: {self.base_url}")
                await page.goto(self.base_url, wait_until='domcontentloaded', timeout=30000)
                
                # Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ theo tag
                await self._crawl_tag_categories(page)
                
                # Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ theo b·∫£ng ch·ªØ c√°i
                await self._crawl_alphabetical_categories(page)
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi truy c·∫≠p trang ch·ªß: {str(e)}")
                self.stats['errors'] += 1
            finally:
                await browser.close()
        
        self.stats['end_time'] = time.time()
        self._save_results()
        self._print_summary()

    async def _crawl_tag_categories(self, page):
        """Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ d·ª±a tr√™n tag"""
        logger.info("üè∑Ô∏è  Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ theo tag...")
        
        try:
            # T√¨m c√°c ph·∫ßn t·ª≠ tag
            tag_selectors = [
                '[class*="grid-nav-list"] li a',
                '.grid-nav-list li a',
                '[data-testid="grid-nav"] li a'
            ]
            
            tag_items = []
            for selector in tag_selectors:
                try:
                    items = await page.query_selector_all(selector)
                    if items:
                        tag_items = items
                        logger.info(f"‚úÖ T√¨m th·∫•y {len(items)} tag items v·ªõi selector: {selector}")
                        break
                except:
                    continue
            
            if not tag_items:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tag items, th·ª≠ selector kh√°c...")
                # Fallback selectors
                fallback_selectors = [
                    'nav a[href*="/"]',
                    '.nav-list a',
                    '[role="navigation"] a'
                ]
                for selector in fallback_selectors:
                    try:
                        items = await page.query_selector_all(selector)
                        if len(items) > 5:  # Ch·ªâ l·∫•y n·∫øu c√≥ ƒë·ªß items
                            tag_items = items[:20]  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
                            logger.info(f"‚úÖ S·ª≠ d·ª•ng fallback selector: {selector}")
                            break
                    except:
                        continue
            
            for item in tag_items:
                try:
                    # L·∫•y title t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau
                    title = None
                    title_selectors = ['h3', 'h2', 'span', '.title']
                    
                    for sel in title_selectors:
                        try:
                            title_elem = await item.query_selector(sel)
                            if title_elem:
                                title = await title_elem.text_content()
                                title = self._clean_text(title)
                                if title:
                                    break
                        except:
                            continue
                    
                    # N·∫øu kh√¥ng t√¨m th·∫•y title trong c√°c element con, l·∫•y text c·ªßa ch√≠nh item
                    if not title:
                        title = await item.text_content()
                        title = self._clean_text(title)
                    
                    # L·∫•y URL
                    url = await item.get_attribute('href')
                    if url:
                        url = urljoin(self.base_url, url)
                        
                        # Validate URL
                        parsed = urlparse(url)
                        if parsed.netloc and 'verywellmind.com' in parsed.netloc:
                            self.results.append({
                                'type': 'tag',
                                'title': title or 'Untitled',
                                'url': url,
                                'category': self._determine_category(title, url)
                            })
                            
                            self.stats['tag_items'] += 1
                            logger.info(f"‚úÖ Thu th·∫≠p tag: {title} -> {url}")
                        else:
                            logger.warning(f"‚ö†Ô∏è URL kh√¥ng h·ª£p l·ªá: {url}")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è B·ªè qua m·ªôt tag item: {str(e)}")
                    self.stats['errors'] += 1
            
            logger.info(f"üè∑Ô∏è  Ho√†n th√†nh thu th·∫≠p {self.stats['tag_items']} tag items")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thu th·∫≠p tag categories: {str(e)}")
            self.stats['errors'] += 1

    async def _crawl_alphabetical_categories(self, page):
        """Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ theo b·∫£ng ch·ªØ c√°i"""
        logger.info("üî§ Thu th·∫≠p c√°c ch·ªß ƒë·ªÅ theo b·∫£ng ch·ªØ c√°i...")
        
        try:
            alpha_selectors = [
                '[class*="alphabetical-nav-list"] li a',
                '.alphabetical-nav-list li a',
                '[data-testid="alphabetical-nav"] li a'
            ]
            
            alpha_items = []
            for selector in alpha_selectors:
                try:
                    items = await page.query_selector_all(selector)
                    if items:
                        alpha_items = items
                        logger.info(f"‚úÖ T√¨m th·∫•y {len(items)} alphabetical items")
                        break
                except:
                    continue
            
            for item in alpha_items:
                try:
                    letter = await item.text_content()
                    letter = self._clean_text(letter)
                    
                    url = await item.get_attribute('href')
                    if url:
                        if not url.startswith('http'):
                            url = urljoin(self.base_url, url)
                        
                        self.results.append({
                            'type': 'alphabetical',
                            'title': f"Topics - {letter}",
                            'url': url,
                            'category': 'alphabetical'
                        })
                        
                        self.stats['alphabetical_items'] += 1
                        logger.info(f"‚úÖ Thu th·∫≠p alphabetical: {letter} -> {url}")
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è B·ªè qua m·ªôt alphabetical item: {str(e)}")
                    self.stats['errors'] += 1
            
            logger.info(f"üî§ Ho√†n th√†nh thu th·∫≠p {self.stats['alphabetical_items']} alphabetical items")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thu th·∫≠p alphabetical categories: {str(e)}")
            self.stats['errors'] += 1

    def _clean_text(self, text):
        """L√†m s·∫°ch text, x·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát"""
        if not text:
            return ""
        
        # X·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = text.replace('\t', ' ')  # Tab th√†nh space
        text = text.replace('\n', ' ')  # Newline th√†nh space
        text = text.replace('\r', ' ')  # Carriage return th√†nh space
        text = text.replace('\u00a0', ' ')  # Non-breaking space
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = ' '.join(text.split())
        
        return text.strip()

    def _determine_category(self, title, url):
        """X√°c ƒë·ªãnh category d·ª±a tr√™n title v√† URL"""
        if not title and not url:
            return 'unknown'
        
        text_to_check = f"{title} {url}".lower()
        
        categories = {
            'therapy': ['therapy', 'treatment', 'counseling', 'psychotherapy'],
            'conditions': ['conditions', 'disorders', 'symptoms', 'diagnosis'],
            'psychology': ['psychology', 'behavior', 'cognitive', 'mental'],
            'wellness': ['wellness', 'self-care', 'healthy', 'lifestyle'],
            'relationships': ['relationships', 'family', 'couples', 'social'],
            'living': ['living', 'daily', 'work', 'stress']
        }
        
        for category, keywords in categories.items():
            if any(keyword in text_to_check for keyword in keywords):
                return category
        
        return 'general'

    def _save_results(self):
        """L∆∞u k·∫øt qu·∫£ v√†o file JSON"""
        self.stats['total_items'] = len(self.results)
        
        output_data = {
            'metadata': {
                'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_items': self.stats['total_items'],
                'stats': self.stats
            },
            'data': self.results
        }
        
        try:
            with open('verywellmind_data.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            logger.info(f"üíæ ƒê√£ l∆∞u {self.stats['total_items']} items v√†o verywellmind_data.json")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u file: {str(e)}")

    def _print_summary(self):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("\n" + "="*60)
        logger.info("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ CRAWL PHASE 1")
        logger.info("="*60)
        logger.info(f"‚è±Ô∏è  Th·ªùi gian th·ª±c hi·ªán: {duration:.2f} gi√¢y")
        logger.info(f"üìà T·ªïng s·ªë items: {self.stats['total_items']}")
        logger.info(f"üè∑Ô∏è  Tag items: {self.stats['tag_items']}")
        logger.info(f"üî§ Alphabetical items: {self.stats['alphabetical_items']}")
        logger.info(f"‚ùå S·ªë l·ªói: {self.stats['errors']}")
        
        # Th·ªëng k√™ theo category
        category_stats = {}
        for item in self.results:
            cat = item.get('category', 'unknown')
            category_stats[cat] = category_stats.get(cat, 0) + 1
        
        logger.info("\nüìä PH√ÇN B·ªê THEO CATEGORY:")
        for category, count in sorted(category_stats.items()):
            logger.info(f"   {category}: {count} items")
        
        logger.info("="*60)

async def main():
    crawler = VerywellMindCrawler()
    await crawler.crawl_categories()

if __name__ == "__main__":
    asyncio.run(main())