import json
import csv
import re
import os
import time
import logging
from urllib.parse import urljoin, urlparse
from playwright.sync_api import sync_playwright
from collections import Counter, defaultdict

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# File c·∫•u h√¨nh ƒë·∫ßu ra
OUTPUT_JSON = "finalcrawl_step2_output.json"
ERROR_LOG = "crawl_errors.csv"
VISITED_LOG = "visited_links.csv"
CATEGORY_LOG = "category_log.csv"
INPUT_FILE = "nimh_step1_articles.json"

# C·∫•u h√¨nh
BASE_URL = "https://www.nimh.nih.gov"
TIMEOUT = 30000  # 30 gi√¢y
DELAY_BETWEEN_REQUESTS = 1  # 1 gi√¢y delay gi·ªØa c√°c request

class WebCrawler:
    def __init__(self):
        self.visited = set()
        self.results = []
        self.errors = []
        self.visited_log = []
        self.category_map = {}
        
    def clean_text(self, text):
        """L√†m s·∫°ch vƒÉn b·∫£n"""
        if not text:
            return ""
        # Lo·∫°i b·ªè k√Ω t·ª± xu·ªëng d√≤ng, tab v√† kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r"[\t\n\r]+", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    
    def is_valid_detail_url(self, url):
        """Ki·ªÉm tra URL c√≥ h·ª£p l·ªá kh√¥ng"""
        try:
            parsed = urlparse(url)
            path_parts = parsed.path.strip("/").split("/")
            return (len(path_parts) >= 2 and 
                   parsed.netloc == "www.nimh.nih.gov" and
                   "health" in parsed.path.lower())
        except Exception:
            return False
    
    def extract_detail_page(self, page, url, category):
        """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ trang chi ti·∫øt"""
        try:
            logger.info(f"Crawling: {url}")
            
            # Navigate to page v·ªõi retry mechanism
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    page.goto(url, timeout=self.TIMEOUT, wait_until="domcontentloaded")
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    logger.warning(f"Retry {attempt + 1} for {url}: {str(e)}")
                    time.sleep(2)
            
            # ƒê·ª£i trang load ho√†n to√†n
            page.wait_for_load_state("networkidle", timeout=10000)
            
            # Tr√≠ch xu·∫•t title
            title = page.title()
            if not title:
                title_elem = page.query_selector("h1")
                title = title_elem.inner_text() if title_elem else "No Title"
            
            # Tr√≠ch xu·∫•t content t·ª´ nhi·ªÅu selector kh√°c nhau
            content = ""
            content_selectors = [
                "article",
                ".content-area", 
                ".main-content",
                ".page-content",
                "main",
                "#content"
            ]
            
            for selector in content_selectors:
                content_elem = page.query_selector(selector)
                if content_elem:
                    content = self.clean_text(content_elem.inner_text())
                    break
            
            # N·∫øu kh√¥ng t√¨m th·∫•y content, l·∫•y to√†n b·ªô body
            if not content:
                body_elem = page.query_selector("body")
                if body_elem:
                    content = self.clean_text(body_elem.inner_text())
            
            # ƒê·∫£m b·∫£o c√≥ n·ªôi dung tr∆∞·ªõc khi l∆∞u
            if content and len(content.strip()) > 50:  # √çt nh·∫•t 50 k√Ω t·ª±
                result = {
                    "title": self.clean_text(title),
                    "url": url,
                    "content": content,
                    "category": category if category else "unknown"
                }
                self.results.append(result)
                self.visited_log.append({"url": url, "status": "success"})
                logger.info(f"‚úÖ Successfully crawled: {title[:50]}...")
            else:
                logger.warning(f"‚ö†Ô∏è No sufficient content found for: {url}")
                self.errors.append({
                    "url": url, 
                    "error": "No sufficient content found",
                    "category": category
                })
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Error crawling {url}: {error_msg}")
            self.errors.append({
                "url": url, 
                "error": error_msg,
                "category": category
            })
    
    def load_input_data(self):
        """ƒê·ªçc file ƒë·∫ßu v√†o t·ª´ b∆∞·ªõc 1"""
        try:
            if not os.path.exists(INPUT_FILE):
                logger.error(f"‚ùå Input file not found: {INPUT_FILE}")
                return []
                
            with open(INPUT_FILE, 'r', encoding='utf-8') as file:
                articles = json.load(file)
                
            logger.info(f"‚úÖ Loaded {len(articles)} articles from {INPUT_FILE}")
            return articles
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Invalid JSON in input file: {e}")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error reading input file: {e}")
            return []
    
    def save_results(self):
        """L∆∞u k·∫øt qu·∫£ v√†o c√°c file output"""
        try:
            # L∆∞u k·∫øt qu·∫£ ch√≠nh v√†o JSON
            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úÖ Saved {len(self.results)} results to {OUTPUT_JSON}")
            
            # L∆∞u log l·ªói
            if self.errors:
                with open(ERROR_LOG, 'w', newline='', encoding='utf-8') as f:
                    fieldnames = ['url', 'error', 'category']
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(self.errors)
                logger.info(f"‚úÖ Saved {len(self.errors)} errors to {ERROR_LOG}")
            
            # L∆∞u log visited
            if self.visited_log:
                with open(VISITED_LOG, 'w', newline='', encoding='utf-8') as f:
                    fieldnames = ['url', 'status']
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(self.visited_log)
                logger.info(f"‚úÖ Saved visited log to {VISITED_LOG}")
            
            # Th·ªëng k√™ danh m·ª•c
            if self.results:
                category_stats = Counter(result['category'] for result in self.results)
                with open(CATEGORY_LOG, 'w', newline='', encoding='utf-8') as f:
                    fieldnames = ['category', 'count']
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    for category, count in category_stats.items():
                        writer.writerow({"category": category, "count": count})
                logger.info(f"‚úÖ Saved category statistics to {CATEGORY_LOG}")
                
        except Exception as e:
            logger.error(f"‚ùå Error saving results: {e}")
    
    def run(self):
        """Ch·∫°y to√†n b·ªô quy tr√¨nh crawling"""
        logger.info("üöÄ Starting web crawling process...")
        
        # B∆∞·ªõc 1: ƒê·ªçc d·ªØ li·ªáu ƒë·∫ßu v√†o
        articles = self.load_input_data()
        if not articles:
            logger.error("‚ùå No input data found. Exiting...")
            return
        
        # C·∫•u h√¨nh Playwright
        try:
            with sync_playwright() as p:
                # Kh·ªüi t·∫°o browser v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-extensions'
                    ]
                )
                
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                )
                
                page = context.new_page()
                
                # B∆∞·ªõc 2: Crawl t·ª´ng b√†i vi·∫øt
                total_articles = len(articles)
                for i, article in enumerate(articles, 1):
                    try:
                        url = article.get('url', '').strip()
                        category = article.get('category', 'unknown')
                        
                        if not url:
                            logger.warning(f"‚ö†Ô∏è Article {i}/{total_articles}: No URL found")
                            continue
                            
                        if url in self.visited:
                            logger.info(f"‚è≠Ô∏è Article {i}/{total_articles}: Already visited {url}")
                            continue
                            
                        self.visited.add(url)
                        logger.info(f"üîÑ Processing article {i}/{total_articles}: {url}")
                        
                        # Crawl trang
                        self.extract_detail_page(page, url, category)
                        
                        # Delay gi·ªØa c√°c request ƒë·ªÉ tr√°nh b·ªã block
                        if i < total_articles:
                            time.sleep(DELAY_BETWEEN_REQUESTS)
                            
                    except Exception as e:
                        logger.error(f"‚ùå Error processing article {i}: {e}")
                        continue
                
                browser.close()
                
        except Exception as e:
            logger.error(f"‚ùå Browser error: {e}")
            return
        
        # B∆∞·ªõc 3: L∆∞u k·∫øt qu·∫£
        self.save_results()
        
        # Th·ªëng k√™ cu·ªëi c√πng
        logger.info("="*50)
        logger.info("üìä CRAWLING SUMMARY")
        logger.info("="*50)
        logger.info(f"‚úÖ Total articles processed: {len(articles)}")
        logger.info(f"‚úÖ Successfully crawled: {len(self.results)}")
        logger.info(f"‚ùå Errors encountered: {len(self.errors)}")
        
        if self.results:
            category_stats = Counter(result['category'] for result in self.results)
            logger.info("üìà Results by category:")
            for category, count in category_stats.most_common():
                logger.info(f"   - {category}: {count}")
        
        logger.info("üéâ Crawling process completed!")

def main():
    """H√†m main"""
    crawler = WebCrawler()
    crawler.TIMEOUT = TIMEOUT
    crawler.run()

if __name__ == '__main__':
    main()